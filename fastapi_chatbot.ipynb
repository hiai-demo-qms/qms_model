{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_TXPZ31No2e"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StNH0326PBtN",
        "outputId": "233a9f8d-8a5e-4d8e-ecfa-654e6bd7fab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n",
            "\u001b[31mERROR: Invalid requirement: 'PyMuPDF!': Expected end or semicolon (after name and no valid version specifier)\n",
            "    PyMuPDF!\n",
            "           ^\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.11\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.11/dist-packages (2.0.41)\n",
            "Collecting pyodbc\n",
            "  Downloading pyodbc-5.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (3.2.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (4.14.0)\n",
            "Downloading pyodbc-5.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (346 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.2/346.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyodbc\n",
            "Successfully installed pyodbc-5.2.0\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.172.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate peft bitsandbytes\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install sentence-transformers faiss-cpu\n",
        "!pip install python-docx PyMuPDF!\n",
        "!pip install PyPDF2\n",
        "!pip install fastapi uvicorn nest-asyncio pyngrok\n",
        "!pip install sqlalchemy pyodbc\n",
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL0Nb1IqN4XT"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AraYH-u9Blp5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import re\n",
        "import gc\n",
        "import sys\n",
        "import json\n",
        "import asyncio\n",
        "import zipfile\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from io import BytesIO\n",
        "\n",
        "from typing import List, Dict, Optional, Tuple, IO\n",
        "\n",
        "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "from PyPDF2 import PdfReader  # hoặc dùng PyPDF2.PdfReader nếu bạn chưa sửa\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_zA3JxQSYqC"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = userdata.get(\"NGROK_AUTH_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhmi_L5fNvzJ"
      },
      "source": [
        "# Connect Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ6lLd02UOIU",
        "outputId": "14640a11-1d9a-47ed-bc47-6da5450af391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Extracting zip file to: /content/iso9001-fine-tuned-model\n",
            "Extraction completed.\n"
          ]
        }
      ],
      "source": [
        "#Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Load Finetuned Model\n",
        "zip_path = '/content/drive/My Drive/iso9001-fine-tuned-model.zip'\n",
        "extract_dir = '/content/iso9001-fine-tuned-model'\n",
        "\n",
        "if not os.path.exists(extract_dir):\n",
        "    print(f\"Extracting zip file to: {extract_dir}\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(\"Extraction completed.\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {extract_dir}. Skipping extraction.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0A4RYui1_UQ"
      },
      "source": [
        "# Gemini Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpO9mCxF1-RA"
      },
      "outputs": [],
      "source": [
        "class ISO9001RAGChatbot:\n",
        "    def __init__(self, gemini_api_key: str):\n",
        "        genai.configure(api_key=gemini_api_key)\n",
        "        self.model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "        self.clause_library = {\n",
        "            \"7.5\": [\n",
        "                {\n",
        "                    \"clause\": \"DOC-01\",\n",
        "                    \"title\": \"Tiêu đề tài liệu\",\n",
        "                    \"description\": \"Tài liệu phải có tiêu đề rõ ràng, phản ánh đúng nội dung và mục đích sử dụng.\",\n",
        "                    \"example\": \"BÁO CÁO tổng kết hoạt động năm học 2023-2024\",\n",
        "                    \"pattern_hint\": r\"(BÁO CÁO|QUY TRÌNH|KẾ HOẠCH|THÔNG BÁO)\"\n",
        "                },\n",
        "                {\n",
        "                    \"clause\": \"DOC-02\",\n",
        "                    \"title\": \"Mã tài liệu\",\n",
        "                    \"description\": \"Tài liệu cần có mã hoặc số được định danh rõ ràng.\",\n",
        "                    \"example\": \"Số: 531/BC-THTTHS\",\n",
        "                    \"pattern_hint\": r\"S[ốo]:?\\s*\\d{1,5}\\s*/\\s*[A-Z\\- ]{2,}\"\n",
        "                },\n",
        "                {\n",
        "                    \"clause\": \"DOC-03\",\n",
        "                    \"title\": \"Ngày ban hành\",\n",
        "                    \"description\": \"Tài liệu phải ghi rõ ngày ban hành để xác định tính hiệu lực.\",\n",
        "                    \"example\": \"ngày 11 tháng 11 năm 2024\",\n",
        "                    \"pattern_hint\": r\"ngày\\s+\\d{1,2}\\s+tháng\\s+\\d{1,2}\\s+năm\\s+\\d{4}\"\n",
        "                },\n",
        "                {\n",
        "                    \"clause\": \"DOC-04\",\n",
        "                    \"title\": \"Người phê duyệt\",\n",
        "                    \"description\": \"Tài liệu phải thể hiện người có thẩm quyền đã xem xét và phê duyệt.\",\n",
        "                    \"example\": \"Hiệu trưởng: Nguyễn Văn A\",\n",
        "                    \"pattern_hint\": r\"(phê duyệt|ký duyệt|Hiệu trưởng|Giám đốc).*?:.*\"\n",
        "                },\n",
        "                {\n",
        "                    \"clause\": \"DOC-05\",\n",
        "                    \"title\": \"Phiên bản\",\n",
        "                    \"description\": \"Phải có chỉ số phiên bản hoặc số hiệu để quản lý các lần sửa đổi.\",\n",
        "                    \"example\": \"Phiên bản: 01 – Ngày cập nhật: 01/01/2024\",\n",
        "                    \"pattern_hint\": r\"(phiên bản|ver|v\\.|số hiệu).*?:?.*\"\n",
        "                },\n",
        "                {\n",
        "                    \"clause\": \"DOC-06\",\n",
        "                    \"title\": \"Người soạn thảo\",\n",
        "                    \"description\": \"Tài liệu nên ghi rõ người soạn thảo để truy vết nguồn gốc và trách nhiệm.\",\n",
        "                    \"example\": \"Người soạn thảo: Trần Thị B\",\n",
        "                    \"pattern_hint\": r\"(Người soạn thảo|Biên soạn|Tác giả).*?:.*\"\n",
        "                },\n",
        "                {\n",
        "                    \"clause\": \"DOC-07\",\n",
        "                    \"title\": \"Phạm vi áp dụng\",\n",
        "                    \"description\": \"Tài liệu cần mô tả rõ phạm vi áp dụng để người dùng hiểu đúng đối tượng điều chỉnh.\",\n",
        "                    \"example\": \"Áp dụng cho toàn bộ giáo viên trường Tiểu học Hương Sơn\",\n",
        "                    \"pattern_hint\": r\"(Phạm vi áp dụng|Áp dụng cho|Áp dụng từ).*\"\n",
        "                },\n",
        "                {\n",
        "                    \"clause\": \"DOC-08\",\n",
        "                    \"title\": \"Biểu mẫu đính kèm\",\n",
        "                    \"description\": \"Tài liệu nên liệt kê các biểu mẫu liên quan, nếu có, để người dùng dễ thực hiện.\",\n",
        "                    \"example\": \"Biểu mẫu BM-01: Phiếu đánh giá giáo viên\",\n",
        "                    \"pattern_hint\": r\"(Biểu mẫu|Phụ lục|BM-\\d{2}).*\"\n",
        "                }\n",
        "            ],\n",
        "            \"9.2\": [\n",
        "                {\n",
        "                    \"clause\": \"AUD-01\",\n",
        "                    \"title\": \"Lập kế hoạch đánh giá nội bộ\",\n",
        "                    \"description\": \"Tổ chức phải lập kế hoạch đánh giá nội bộ định kỳ, đảm bảo bao phủ toàn bộ hệ thống.\"\n",
        "                },\n",
        "                {\n",
        "                    \"clause\": \"AUD-02\",\n",
        "                    \"title\": \"Tiêu chí và phạm vi đánh giá\",\n",
        "                    \"description\": \"Mỗi cuộc đánh giá phải xác định rõ tiêu chí, phạm vi và phương pháp tiến hành.\"\n",
        "                },\n",
        "                {\n",
        "                    \"clause\": \"AUD-03\",\n",
        "                    \"title\": \"Chọn đánh giá viên phù hợp\",\n",
        "                    \"description\": \"Người đánh giá phải độc lập với quá trình được đánh giá và có đủ năng lực.\"\n",
        "                },\n",
        "                {\n",
        "                    \"clause\": \"AUD-04\",\n",
        "                    \"title\": \"Ghi nhận kết quả đánh giá\",\n",
        "                    \"description\": \"Kết quả đánh giá phải được ghi lại đầy đủ và minh bạch để đối chiếu và cải tiến.\"\n",
        "                },\n",
        "                {\n",
        "                    \"clause\": \"AUD-05\",\n",
        "                    \"title\": \"Hành động khắc phục\",\n",
        "                    \"description\": \"Các phát hiện cần hành động khắc phục phải được xử lý kịp thời và theo dõi hiệu quả.\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Gộp tất cả vào 'all'\n",
        "        self.clause_library[\"all\"] = self.clause_library[\"7.5\"] + self.clause_library[\"9.2\"]\n",
        "\n",
        "    def extract_text_from_pdf(self, file: BytesIO) -> str:\n",
        "        reader = PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "        return text.strip()\n",
        "\n",
        "    def retrieve_clauses(self, document_text: str, clause_group: str = \"7.5\", top_k: int = 10) -> List[Dict]:\n",
        "        all_clauses = self.clause_library.get(clause_group, [])\n",
        "        return all_clauses[:top_k]\n",
        "\n",
        "    def build_prompt(self, content: str, clauses: List[Dict]) -> str:\n",
        "        clause_list = \"\\n\".join([f'{c[\"clause\"]}: {c[\"title\"]}' for c in clauses])\n",
        "        return f\"\"\"\n",
        "Bạn là chuyên gia đánh giá tài liệu theo ISO 9001:2015.\n",
        "\n",
        "Nội dung tài liệu:\n",
        "\\\"\\\"\\\"\n",
        "{content[:3000]}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Tiêu chí đánh giá:\n",
        "{clause_list}\n",
        "\n",
        "Đánh giá tài liệu, đảm bảo không bỏ sót các tiêu chí, trả về JSON với định dạng:\n",
        "{{\n",
        "  \"compliance_results\": {{\n",
        "    \"DOC-01\": {{\n",
        "      \"title\": \"...\",\n",
        "      \"score\": 100,\n",
        "      \"status\": \"Đạt\" hoặc \"Không đạt\",\n",
        "      \"evidences\": [\"...\"]\n",
        "    }},\n",
        "    ...\n",
        "  }}\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    def call_gemini(self, prompt: str) -> dict:\n",
        "        response = self.model.generate_content(prompt)\n",
        "        raw_text = response.text.strip()\n",
        "        try:\n",
        "            if raw_text.startswith(\"```json\") or raw_text.startswith(\"```\"):\n",
        "              raw_text = re.sub(r\"^```(?:json)?\", \"\", raw_text)\n",
        "              raw_text = re.sub(r\"```$\", \"\", raw_text)\n",
        "              raw_text = raw_text.strip()\n",
        "\n",
        "            return json.loads(raw_text)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Lỗi xử lý Gemini: {e}\\nPhản hồi:\\n{response.text}\")\n",
        "\n",
        "    def process_document(self, file_stream: BytesIO) -> dict:\n",
        "        content = self.extract_text_from_pdf(file_stream)\n",
        "        clauses = self.retrieve_clauses(content)\n",
        "        prompt = self.build_prompt(content, clauses)\n",
        "        result = self.call_gemini(prompt)\n",
        "        return result\n",
        "\n",
        "    def generate_compliance_summary(self, compliance_results: dict) -> Tuple[float, List[Tuple[str, dict]]]:\n",
        "        total, count = 0, 0\n",
        "        results = []\n",
        "        for clause, result in compliance_results.items():\n",
        "            total += result[\"score\"]\n",
        "            count += 1\n",
        "            results.append((clause, result))\n",
        "        avg_score = total / count if count else 0\n",
        "        return avg_score, results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjJ32810OSo1"
      },
      "source": [
        "# Finetuned Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KkM-A-pOlfU"
      },
      "outputs": [],
      "source": [
        "class ISO9001ChatBot:\n",
        "    def __init__(self, model_path: str):\n",
        "        self.base_model = \"Viet-Mistral/Vistral-7B-Chat\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model, trust_remote_code=True)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        base = AutoModelForCausalLM.from_pretrained(\n",
        "            self.base_model,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        from peft import PeftModel\n",
        "        self.model = PeftModel.from_pretrained(base, model_path)\n",
        "        self.model.eval()\n",
        "\n",
        "    def _classify_question(self, question: str) -> str:\n",
        "        # Định nghĩa nội dung trao đổi theo format chat\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Bạn là một trợ lý chuyên đánh giá câu hỏi người dùng về quản lý chất lượng (QMS).\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Phân loại câu hỏi sau thành một trong hai loại:\n",
        "\n",
        "    1. document_search — nếu người dùng muốn tìm, xem hoặc yêu cầu một tài liệu cụ thể (như biểu mẫu, file, SOP, quy trình...).\n",
        "    2. normal_question — nếu người dùng đang hỏi về kiến thức, khái niệm, yêu cầu của ISO 9001:2015.\n",
        "\n",
        "    Chỉ trả lời duy nhất một từ: document_search hoặc normal_question.\n",
        "\n",
        "    Câu hỏi: \"{question}\"\n",
        "    Loại:\"\"\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Áp dụng chat template của model\n",
        "        prompt = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        # Tokenize prompt theo chuẩn của Vistral\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=10,\n",
        "                do_sample=False,  # sinh chắc chắn\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        result = self.tokenizer.decode(\n",
        "            output[0][inputs[\"input_ids\"].shape[1]:],\n",
        "            skip_special_tokens=True\n",
        "        ).strip().lower()\n",
        "\n",
        "        # Phân loại kết quả\n",
        "        if \"document_search\" in result:\n",
        "            return \"document_search\"\n",
        "        elif \"normal_question\" in result:\n",
        "            return \"normal_question\"\n",
        "        else:\n",
        "            return \"unknown\"\n",
        "\n",
        "    def _extract_keywords(self, question: str) -> List[str]:\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Bạn là trợ lý QMS. Nhiệm vụ của bạn là trích xuất từ khóa tìm kiếm từ câu hỏi tiếng Việt.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Câu hỏi: \"{question}\"\n",
        "\n",
        "        Hãy trích xuất các từ khóa chính (dưới dạng tiếng Việt), phân cách bằng dấu phẩy.\n",
        "        Không dịch sang tiếng Anh. Chỉ liệt kê từ khóa, không giải thích.\n",
        "        \"\"\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        print(prompt)\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=30,\n",
        "                do_sample=False,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        result = self.tokenizer.decode(output[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "        return [kw.strip() for kw in result.split(\",\") if kw.strip()]\n",
        "\n",
        "    def _chat_sync(self, question: str) -> str:\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Bạn là trợ lý ISO 9001:2015. Dựa trên ngữ cảnh cuộc trò chuyện, hãy trả lời câu hỏi mới nhất một cách chi tiết, chính xác và thực tế.\"\n",
        "            }\n",
        "        ] + [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"{question}\n",
        "\n",
        "    Trả lời: \"\"\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Tạo prompt chat đúng định dạng\n",
        "        prompt = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        print(prompt)\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048, padding=True)\n",
        "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=300,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        return self.tokenizer.decode(output[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    async def chat(self, question: str) -> str:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        return await loop.run_in_executor(None, self._chat_sync, question)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gutyn06tObN3"
      },
      "source": [
        "# API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWAgZ1uwA3xK",
        "outputId": "0ba4b187-f2ed-4256-dd0e-956dd4fe67e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Câu hỏi: Tổ chức có nên lưu trữ tài liệu giấy không?\n",
            "Phân loại: related\n",
            "\n",
            "Câu hỏi: AI sẽ thay thế con người trong tương lai?\n",
            "Phân loại: unrelated\n",
            "\n",
            "Câu hỏi: Quy trình phê duyệt tài liệu diễn ra như thế nào?\n",
            "Phân loại: related\n",
            "\n",
            "Câu hỏi: Hôm nay thời tiết thế nào?\n",
            "Phân loại: unrelated\n",
            "\n",
            "Câu hỏi: Làm sao để kiểm soát thay đổi tài liệu hiệu quả?\n",
            "Phân loại: related\n",
            "\n",
            "Câu hỏi: giải thích về NCR\n",
            "Phân loại: related\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Danh sách từ khóa QMS\n",
        "qms_keywords = [\n",
        "    \"iso 9001\", \"hệ thống quản lý chất lượng\", \"qms\", \"tiêu chuẩn iso\", \"iso\",\n",
        "    \"đánh giá nội bộ\", \"đánh giá chất lượng\", \"kiểm tra chất lượng\", \"cải tiến liên tục\",\n",
        "    \"kiểm soát chất lượng\", \"đào tạo chất lượng\", \"rà soát quản lý\", \"phân tích nguyên nhân\",\n",
        "    \"hành động khắc phục\", \"hành động phòng ngừa\", \"báo cáo không phù hợp\",\n",
        "    \"quản lý rủi ro\", \"giám sát và đo lường\", \"Điều khoản\", \"Sổ tay hướng dẫn\",\n",
        "    \"quản lý tài liệu\", \"kiểm soát tài liệu\", \"lưu trữ tài liệu\", \"phiên bản tài liệu\",\n",
        "    \"bản phát hành\", \"phê duyệt tài liệu\", \"thay đổi tài liệu\", \"phân phối tài liệu\",\n",
        "    \"thu hồi tài liệu\", \"tài liệu hướng dẫn\", \"biểu mẫu\", \"quy trình\", \"quy định\",\n",
        "    \"hồ sơ chất lượng\", \"tài liệu viết tay\", \"tài liệu điện tử\", \"hướng dẫn công việc\",\n",
        "    \"kiểm soát thay đổi\", \"ghi nhận tài liệu\", \"truy cập tài liệu\", \"hướng dẫn\",\n",
        "    \"trách nhiệm\", \"quyền hạn\", \"người phụ trách\", \"bộ phận quản lý chất lượng\",\n",
        "    \"phân công nhiệm vụ\", \"người soạn thảo\", \"người phê duyệt\", \"người sử dụng tài liệu\",\n",
        "    \"đào tạo nhân viên\", \"giám sát\", \"điều khoản\", \"tài liệu\", \"hướng dẫn\",\n",
        "    \"đánh giá tài liệu\", \"kiểm tra hiệu quả\", \"đo lường hiệu quả\", \"phản hồi\",\n",
        "    \"cải tiến tài liệu\", \"kiểm toán nội bộ\", \"báo cáo kiểm toán\", \"khắc phục sai sót\",\n",
        "    \"rủi ro\", \"quản lý rủi ro\", \"biện pháp phòng ngừa\", \"hành động khắc phục\",\n",
        "    \"điều tra nguyên nhân\", \"xử lý sự cố\", \"ncr\", \"hướng dẫn\", \"viết\", \"phiên bản\", \"loại\",\n",
        "    \"hệ thống iso\", \"quy trình làm việc\", \"tiêu chí chất lượng\", \"bảng kiểm tra\",\n",
        "    \"hồ sơ kiểm tra\", \"biên bản họp\", \"chứng nhận iso\", \"mẫu biểu\", \"biểu mẫu\", \"quy trình\",\n",
        "    \"quy định\", \"sổ tay chất lượng\", \"sổ tay hướng dẫn\", \"mục\", \"nội dung\",\n",
        "    \"hồ sơ chất lượng\", \"tài liệu viết tay\", \"tài liệu điện tử\", \"hướng dẫn công việc\",\n",
        "    \"kiểm soát thay đổi\", \"ghi nhận tài liệu\", \"truy cập tài liệu\"\n",
        "]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Chuyển về chữ thường\n",
        "    text = text.lower()\n",
        "    # Loại bỏ dấu câu\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # Loại bỏ khoảng trắng thừa\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def classify_question_keyword_based(question):\n",
        "    question_processed = preprocess_text(question)\n",
        "    for keyword in qms_keywords:\n",
        "        if keyword in question_processed:\n",
        "            return \"related\"\n",
        "    return \"unrelated\"\n",
        "\n",
        "# Ví dụ test\n",
        "questions = [\n",
        "    \"Tổ chức có nên lưu trữ tài liệu giấy không?\",\n",
        "    \"AI sẽ thay thế con người trong tương lai?\",\n",
        "    \"Quy trình phê duyệt tài liệu diễn ra như thế nào?\",\n",
        "    \"Hôm nay thời tiết thế nào?\",\n",
        "    \"Làm sao để kiểm soát thay đổi tài liệu hiệu quả?\",\n",
        "    \"giải thích về NCR\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    result = classify_question_keyword_based(q)\n",
        "    print(f\"Câu hỏi: {q}\\nPhân loại: {result}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5q7_D-hbI6c"
      },
      "outputs": [],
      "source": [
        "# chatbot = ISO9001ChatBot(\"./iso9001-fine-tuned-model/content/iso9001-fine-tuned-model\")\n",
        "rag_chatbot = ISO9001RAGChatbot(gemini_api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywo8JeXJkTUA"
      },
      "outputs": [],
      "source": [
        "# rag_chatbot.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwIkUgvMO4DD",
        "outputId": "bd0065d3-432a-4942-d9f1-98f6f5b41e9f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-93' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# Cho phép gọi từ frontend khác domain nếu cần\n",
        "app = FastAPI(title=\"ISO 9001 Chatbot API\")\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "class ClauseResult(BaseModel):\n",
        "    clause: str\n",
        "    title: str\n",
        "    status: str\n",
        "    score: float\n",
        "    evidences: List[str] = []\n",
        "\n",
        "class AnalyzeResponse(BaseModel):\n",
        "    score: float\n",
        "    clause_results: List[ClauseResult]\n",
        "\n",
        "class KeywordResponse(BaseModel):\n",
        "    keywords: List[str]\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "class ChatResponse(BaseModel):\n",
        "    response: str\n",
        "\n",
        "class ChatMessage(BaseModel):\n",
        "    role: str\n",
        "    content: str\n",
        "\n",
        "class ChatFullRequest(BaseModel):\n",
        "    question: str\n",
        "    history_messages: List[ChatMessage]\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"ISO 9001 Chatbot API is running\"}\n",
        "\n",
        "@app.post(\"/get-label\", response_model=ChatResponse)\n",
        "async def chat(req: ChatRequest):\n",
        "    if not req.question.strip():\n",
        "        raise HTTPException(status_code=400, detail=\"Question is required.\")\n",
        "\n",
        "    try:\n",
        "        print(\"Calling chatbot...\")\n",
        "        if(classify_question_keyword_based(req.question) == \"unrelated\"):\n",
        "            response = \"unknown\"\n",
        "        else:\n",
        "            response = chatbot._classify_question(req.question)\n",
        "\n",
        "        return {\"response\": response}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/extract-keywords\", response_model=KeywordResponse)\n",
        "async def extract_keywords(req: ChatRequest):\n",
        "    if not req.question.strip():\n",
        "        raise HTTPException(status_code=400, detail=\"Question is required.\")\n",
        "\n",
        "    try:\n",
        "        keywords = chatbot._extract_keywords(req.question)\n",
        "        return {\"keywords\": keywords}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/chat\", response_model=ChatResponse)\n",
        "async def chat(req: ChatRequest):\n",
        "    if not req.question.strip():\n",
        "        raise HTTPException(status_code=400, detail=\"Question is required.\")\n",
        "\n",
        "    try:\n",
        "        response = chatbot._chat_sync(req.question)\n",
        "        return {\"response\": response}\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/analyze\", response_model=AnalyzeResponse)\n",
        "async def analyze(document: UploadFile = File(...)):\n",
        "    try:\n",
        "        # Đọc nội dung file PDF\n",
        "        content = await document.read()\n",
        "        file_stream = BytesIO(content)\n",
        "        file_stream.name = document.filename\n",
        "\n",
        "        # Gọi xử lý tài liệu bằng Gemini + RAG\n",
        "        doc_results = rag_chatbot.process_document(file_stream)\n",
        "\n",
        "        # Sinh điểm trung bình và kết quả từng tiêu chí\n",
        "        score, clause_results_raw = rag_chatbot.generate_compliance_summary(\n",
        "            doc_results[\"compliance_results\"]\n",
        "        )\n",
        "\n",
        "        # Chuyển thành danh sách ClauseResult\n",
        "        clause_results = [\n",
        "            ClauseResult(\n",
        "                clause=clause,\n",
        "                title=data[\"title\"],\n",
        "                status=data[\"status\"],\n",
        "                score=data[\"score\"],\n",
        "                evidences=data.get(\"evidences\", [])\n",
        "            )\n",
        "            for clause, data in clause_results_raw\n",
        "        ]\n",
        "\n",
        "        return AnalyzeResponse(score=score, clause_results=clause_results)\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Lỗi xử lý tài liệu: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSmdE9dyOepl"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WQxWYMdGPY0z",
        "outputId": "3f89f1f8-7cf4-456f-ba1d-8f02c79dda1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://ample-wildly-parrot.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [2814]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-71' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-82' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2001:ee0:4f0e:f780:535:3ca4:d709:e332:0 - \"POST /analyze HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from google.colab import userdata\n",
        "\n",
        "# Áp dụng patch cho asyncio để chạy trong notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "ngrok.set_auth_token(os.environ[\"NGROK_AUTH_TOKEN\"])\n",
        "\n",
        "# Mở cổng 8000 ra internet\n",
        "public_url = ngrok.connect(addr=8000, domain=\"ample-wildly-parrot.ngrok-free.app\")\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# Chạy server\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}